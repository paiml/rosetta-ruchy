#!/usr/bin/env ruchy

use std::time::Instant
use std::process::Command
use std::collections::HashMap
use std::fs::File
use std::io::Write

struct BenchmarkResult {
    language: String,
    implementation: String,
    size: usize,
    duration_ms: f64,
    memory_mb: f64,
}

struct BenchmarkConfig {
    sizes: Vec<usize>,
    iterations: usize,
    warmup_iterations: usize,
    patterns: Vec<String>,
}

impl Default for BenchmarkConfig {
    fun default() -> Self {
        Self {
            sizes: vec![100, 1000, 10000, 100000],
            iterations: 1000,
            warmup_iterations: 100,
            patterns: vec![
                "random".to_string(),
                "sorted".to_string(), 
                "reverse".to_string(),
                "duplicates".to_string()
            ],
        }
    }
}

fun benchmark_ruchy_implementation(size: usize, pattern: &str) -> f64 {
    let start = Instant::now()
    
    let output = Command::new("ruchy")
        .args(&["run", "implementations/ruchy/quicksort.ruchy", "--bench", &size.to_string(), pattern])
        .output()
        .expect("Failed to run Ruchy benchmark")
    
    let duration = start.elapsed()
    duration.as_secs_f64() * 1000.0
}

fun benchmark_rust_implementation(size: usize, pattern: &str) -> f64 {
    let start = Instant::now()
    
    let output = Command::new("cargo")
        .args(&["run", "--release", "--manifest-path", "implementations/rust/Cargo.toml", "--", "--bench", &size.to_string(), pattern])
        .output()
        .expect("Failed to run Rust benchmark")
    
    let duration = start.elapsed()
    duration.as_secs_f64() * 1000.0
}

fun benchmark_python_implementation(size: usize, pattern: &str) -> f64 {
    let start = Instant::now()
    
    let output = Command::new("python3")
        .args(&["implementations/python/quicksort.py", "--bench", &size.to_string(), pattern])
        .output()
        .expect("Failed to run Python benchmark")
    
    let duration = start.elapsed()
    duration.as_secs_f64() * 1000.0
}

fun benchmark_javascript_implementation(size: usize, pattern: &str) -> f64 {
    let start = Instant::now()
    
    let output = Command::new("node")
        .args(&["implementations/javascript/quicksort.js", "--bench", &size.to_string(), pattern])
        .output()
        .expect("Failed to run JavaScript benchmark")
    
    let duration = start.elapsed()
    duration.as_secs_f64() * 1000.0
}

fun benchmark_go_implementation(size: usize, pattern: &str) -> f64 {
    let start = Instant::now()
    
    let output = Command::new("go")
        .args(&["run", "implementations/go/quicksort.go", "--bench", &size.to_string(), pattern])
        .output()
        .expect("Failed to run Go benchmark")
    
    let duration = start.elapsed()
    duration.as_secs_f64() * 1000.0
}

fun benchmark_c_implementation(size: usize, pattern: &str) -> f64 {
    let start = Instant::now()
    
    let output = Command::new("implementations/c/quicksort")
        .args(&["--bench", &size.to_string(), pattern])
        .output()
        .expect("Failed to run C benchmark")
    
    let duration = start.elapsed()
    duration.as_secs_f64() * 1000.0
}

fun run_benchmark_suite() -> Vec<BenchmarkResult> {
    let config = BenchmarkConfig::default()
    let mut results = Vec::new()
    
    let implementations = vec![
        ("ruchy", benchmark_ruchy_implementation),
        ("rust", benchmark_rust_implementation),
        ("python", benchmark_python_implementation),
        ("javascript", benchmark_javascript_implementation),
        ("go", benchmark_go_implementation),
        ("c", benchmark_c_implementation),
    ]
    
    println("🚀 Starting benchmark suite...")
    println("Configuration:")
    println("  Sizes: {:?}", config.sizes)
    println("  Iterations: {}", config.iterations)
    println("  Warmup: {}", config.warmup_iterations)
    println("  Patterns: {:?}", config.patterns)
    println()
    
    for (lang, benchmark_fn) in implementations {
        println("Benchmarking {} implementation...", lang)
        
        for &size in &config.sizes {
            for pattern in &config.patterns {
                print!("  {} elements ({}): ", size, pattern)
                
                let mut durations = Vec::new()
                
                // Warmup runs
                for _ in 0..config.warmup_iterations {
                    benchmark_fn(size, pattern)
                }
                
                // Actual benchmark runs
                for _ in 0..config.iterations {
                    let duration = benchmark_fn(size, pattern)
                    durations.push(duration)
                }
                
                // Calculate statistics
                durations.sort_by(|a, b| a.partial_cmp(b).unwrap())
                let median = durations[durations.len() / 2]
                let mean = durations.iter().sum::<f64>() / durations.len() as f64
                let min = durations[0]
                let max = durations[durations.len() - 1]
                
                println!("median: {:.2}ms, mean: {:.2}ms, min: {:.2}ms, max: {:.2}ms", 
                        median, mean, min, max)
                
                results.push(BenchmarkResult {
                    language: lang.to_string(),
                    implementation: format!("{}-{}", lang, pattern),
                    size,
                    duration_ms: median,
                    memory_mb: 0.0, // TODO: Implement memory measurement
                })
            }
        }
        
        println()
    }
    
    results
}

fun generate_performance_report(results: &[BenchmarkResult]) {
    let mut file = File::create("results/performance_report.md").unwrap()
    
    writeln!(file, "# Quicksort Performance Report").unwrap()
    writeln!(file, "").unwrap()
    writeln!(file, "Generated: {}", chrono::Utc::now().format("%Y-%m-%d %H:%M:%S UTC")).unwrap()
    writeln!(file, "").unwrap()
    
    // Group by size
    let mut by_size: HashMap<usize, Vec<&BenchmarkResult>> = HashMap::new()
    for result in results {
        by_size.entry(result.size).or_insert_with(Vec::new).push(result)
    }
    
    for &size in &[100, 1000, 10000, 100000] {
        if let Some(size_results) = by_size.get(&size) {
            writeln!(file, "## {} Elements", size).unwrap()
            writeln!(file, "").unwrap()
            writeln!(file, "| Language | Implementation | Time (ms) | Relative to Rust |").unwrap()
            writeln!(file, "|----------|----------------|-----------|------------------|").unwrap()
            
            let rust_baseline = size_results.iter()
                >> find(|r| r.language == "rust")
                >> map(|r| r.duration_ms)
                >> unwrap_or(1.0)
            
            for result in size_results {
                let relative = result.duration_ms / rust_baseline
                writeln!(file, "| {} | {} | {:.2} | {:.2}x |", 
                        result.language, 
                        result.implementation, 
                        result.duration_ms,
                        relative).unwrap()
            }
            
            writeln!(file, "").unwrap()
        }
    }
    
    // Performance analysis
    writeln!(file, "## Analysis").unwrap()
    writeln!(file, "").unwrap()
    
    let ruchy_results = results.iter().filter(|r| r.language == "ruchy").collect::<Vec<_>>()
    let rust_results = results.iter().filter(|r| r.language == "rust").collect::<Vec<_>>()
    
    if !ruchy_results.is_empty() && !rust_results.is_empty() {
        let ruchy_avg = ruchy_results.iter().map(|r| r.duration_ms).sum::<f64>() / ruchy_results.len() as f64
        let rust_avg = rust_results.iter().map(|r| r.duration_ms).sum::<f64>() / rust_results.len() as f64
        let ratio = ruchy_avg / rust_avg
        
        writeln!(file, "- **Ruchy vs Rust**: {:.2}x (target: ≤1.05x)", ratio).unwrap()
        
        if ratio <= 1.05 {
            writeln!(file, "- ✅ **Performance Target Met**: Ruchy is within 5% of Rust performance").unwrap()
        } else {
            writeln!(file, "- ❌ **Performance Target Missed**: Ruchy is {:.1}% slower than target", (ratio - 1.0) * 100.0).unwrap()
        }
    }
    
    println("📊 Performance report generated: results/performance_report.md")
}

fun run_ruchy_advanced_analysis() {
    println("🔍 Running Ruchy advanced analysis...")
    
    let analysis_commands = vec![
        ("AST Analysis", vec!["ruchy", "ast", "implementations/ruchy/quicksort.ruchy", "--format", "json"]),
        ("Provability Check", vec!["ruchy", "provability", "implementations/ruchy/quicksort.ruchy"]),
        ("Complexity Analysis", vec!["ruchy", "runtime", "implementations/ruchy/quicksort.ruchy"]),
        ("Quality Score", vec!["ruchy", "score", "implementations/ruchy/quicksort.ruchy"]),
    ]
    
    for (name, cmd) in analysis_commands {
        println!("  Running {}...", name)
        
        let output = Command::new(&cmd[0])
            .args(&cmd[1..])
            .output()
            .expect(&format!("Failed to run {}", name))
        
        if output.status.success() {
            println!("    ✅ {} completed", name)
            
            let output_file = format!("results/{}.txt", name.to_lowercase().replace(" ", "_"))
            let mut file = File::create(&output_file).unwrap()
            file.write_all(&output.stdout).unwrap()
        } else {
            println!("    ❌ {} failed", name)
            println!("    Error: {}", String::from_utf8_lossy(&output.stderr))
        }
    }
}

fun main() {
    std::fs::create_dir_all("results").expect("Failed to create results directory")
    
    println("🏁 Quicksort Benchmark Suite")
    println("==============================")
    println()
    
    // Run Ruchy advanced analysis first
    run_ruchy_advanced_analysis()
    println()
    
    // Run performance benchmarks
    let results = run_benchmark_suite()
    
    // Generate report
    generate_performance_report(&results)
    
    println()
    println("🎉 Benchmark suite completed!")
    println("Results saved in results/ directory")
}